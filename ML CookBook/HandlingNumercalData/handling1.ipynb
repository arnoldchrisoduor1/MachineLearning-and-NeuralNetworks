{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling / Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons for Feature Scaling include:-\n",
    "\n",
    "#### Improved Convergence.\n",
    "- Algorithms like gradient descent converge faster when features are scaled, unscaled features cause gradients to be very different in magnitude, leading to zigzag paths towards the minimum during oprimization.\n",
    "\n",
    "#### Equal Treatment of Features:\n",
    "- Algorithms such as KNN and SVMs are sensitive allowing for larger scales to disproportionately influence the results, leading to biased models.\n",
    "- Rescaling ensures that each feature contributes equally to the model.\n",
    "\n",
    "#### Enhanced Model Interpretability:\n",
    "- Model coefficients and weights can be more easily interpreted as they correspond to features of similar magnitude.\n",
    "\n",
    "#### Stabilized Training:\n",
    "- Neural Networks v=benefit from feature scaling as it helps prevent the network from becoming unstable due to large values in the input data.\n",
    "\n",
    "#### Compatibility with Regularizatio:\n",
    "- Regularization techniques like Lasso / rigde regression are affected by feature scales, regularization ensures tha regularization penalties are applied uniformly across all features.\n",
    "\n",
    "- Common rescaling methods include min-ma scaling, z-score normalization and max-abs scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28578564],\n",
       "       [0.35714286],\n",
       "       [0.42857143],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a feature.\n",
    "feature = np.array([[-500.5],\n",
    "                    [-100.0],\n",
    "                    [0],\n",
    "                    [100.1],\n",
    "                    [900.9]])\n",
    "\n",
    "# create scaler\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1)) \n",
    "# goal of the Min-Max scaling is to transform the features so they are within a specific range i.e (0, 1) in this case.\n",
    "\n",
    "# Scaling he feature.\n",
    "scaled_feature = minmax_scale.fit_transform(feature)\n",
    "# if we used fit only the min and max calculations would have been done and stores separately.\n",
    "# the transform is then used to change the original values and rescale them\n",
    "# fit_transform does both at the same time.\n",
    "\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score / Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76058269],\n",
       "       [-0.54177196],\n",
       "       [-0.35009716],\n",
       "       [-0.32271504],\n",
       "       [ 1.97516685]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardizing a Feature / Z-score.\n",
    "\n",
    "x = np.array([[-1000.1],\n",
    "[-200.2],\n",
    "[500.5],\n",
    "[600.6],\n",
    "[9000.9]])\n",
    "\n",
    "# Create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# features will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "# Transform the feature\n",
    "standardized = scaler.fit_transform(x)\n",
    "\n",
    "# Show feature\n",
    "standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0\n",
      "Standard deviation: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Print mean and standard deviation\n",
    "print(\"Mean:\", round(standardized.mean()))\n",
    "print(\"Standard deviation:\", standardized.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standard Scaler works best for PCA but neural networks are better of with min-max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87387612],\n",
       "       [-0.875     ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [10.61488511]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar to standard scaler only it uses the median and the quartile range to counter the negative effects of significant outliers.\n",
    "\n",
    "# Create scaler\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Transform feature\n",
    "rubust = robust_scaler.fit_transform(x)\n",
    "\n",
    "\n",
    "rubust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We do this when we want to rescale the feature values of observations to have unit norm ( a total length of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.07405353, 0.99725427],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Euclideoan normalization.\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# Create feature matrix\n",
    "features = np.array([[0.5, 0.5],\n",
    "[1.1, 3.4],\n",
    "[1.5, 20.2],\n",
    "[1.63, 34.4],\n",
    "[10.9, 3.3]])\n",
    "# Create normalizer\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "# Transform feature matrix\n",
    "new_features = normalizer.transform(features)\n",
    "\n",
    "new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.24444444, 0.75555556],\n",
       "       [0.06912442, 0.93087558],\n",
       "       [0.04524008, 0.95475992],\n",
       "       [0.76760563, 0.23239437]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Manhattan norm (L1):\n",
    "\n",
    "features_l1_norm = Normalizer(norm=\"l1\").transform(features)\n",
    "\n",
    "features_l1_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating polynomial and interraction features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial features are often created when we want to include the notion that there exists a nonlinear r/ship between the features and the target.\n",
    "- When we suspect a relationshipis non-linear we can encode that non-constant effect in a feature, x, by generating that feature's higher-order forms.\n",
    "\n",
    "- Interrcation features occur if the effects of each feature on the target are dependent on each other, we encode that relashioship by including an interraction feature that is the product of the individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create feature matrix\n",
    "features = np.array([[2, 3],\n",
    "                    [2, 3],\n",
    "                    [2, 3]])\n",
    "\n",
    "# create a PolyomialFeatures object.\n",
    "polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# create polynomial features.\n",
    "polynomial_interaction.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 6.],\n",
       "       [2., 3., 6.],\n",
       "       [2., 3., 6.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default PolynomialFeatures includes interaction features.\n",
    "# We can restrict the features created to only interaction features by:\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations when using polynomial and interation\n",
    "\n",
    "- Adding polynomial and interaction features increases the number of features , which can lead to overfitting if not handled carefully, especially in models with a large number of original features.\n",
    "\n",
    "- Computational costs.\n",
    "\n",
    "- Often when using polynomial and interaction features,regularization techniques like Ridge or Lasso regression are used to prevent overfitting by penalizing large coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 13],\n",
       "       [12, 13],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This approach is commonly used when we want to make a custome transformation to one or more features:\n",
    "\n",
    "# In Scikit-learn we can use FunctionTransformer to apply a function to a set of features.\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Create feature matrix\n",
    "features = np.array([[2, 3],\n",
    "                    [2, 3],\n",
    "                    [2, 3]])\n",
    "\n",
    "# Define a simple function.\n",
    "def add_ten(x: int) -> int:\n",
    "    return x + 10\n",
    "\n",
    "# create the transformer:\n",
    "ten_transformer = FunctionTransformer(add_ten)\n",
    "\n",
    "# Transformer feature matrix:\n",
    "ten_transformer.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0         12         13\n",
       "1         12         13\n",
       "2         12         13"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can create the same transformetion in pandas as:\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
    "# Apply function\n",
    "df.apply(add_ten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
